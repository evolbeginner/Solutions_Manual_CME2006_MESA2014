**Solutions.**

As follows two solutions are provided, the first showing that the
expectation of the two probabilities are equal, and the second adopting
a way similar to Problem 8.4's solution. Note that the two ways of
solving the problem is somehow different in how to define "equally wrong
models", and our feeling is that the way it is defined in Solution 2 may
be closer to what Ziheng hopes to seek.

**Solution 1.**

Denote the variable $X$ as the mean of $X_{1},X_{2},\ldots X_{n}$.
According to the problem statement,

$${P\left( H_{1}|X = x \right) = \frac{0.5 \times \mu_{1}^{- 1}\ e^{- \mu_{1}^{- 1}x}}{0.5 \times \mu_{1}^{- 1}e^{- \mu_{1}^{- 1}x} + 0.5 \times \mu_{2}^{- 1}e^{- \mu_{2}^{- 1}x}}
}{= \frac{1}{1 + \frac{\mu_{1}}{\mu_{2}}e^{\left( \frac{1}{\mu_{1}} - \frac{1}{\mu_{2}} \right)x}}.}$$

Denote $k$ as any positive number. It follows that

$$
\begin{align*}
& \left\{ \begin{array}{r}
P\left( H_{1} \middle| \mu_{0} + k \right) + P\left( H_{2} \middle| \mu_{0} + k \right) = 1 \\
P\left( H_{1} \middle| \mu_{0} - k \right) + P\left( H_{2} \middle| \mu_{0} - k \right) = 1
\end{array} \right. .\#(8.1)
\end{align*}
$$

Consider the more general case where
$\mu_{0} = \log\left( \frac{\frac{\mu_{2}}{\mu_{1}}}{\mu_{1}^{- 1} - \mu_{2}^{- 1}} \right)$.
Therefore, we have

<p align=center>
  <img src=img/8.5-0.png height="50%" width="50%">
</p>

Actually, at the third-to-last step, if you are familiar with logistic
regression or some basics of neural network, you should be able to
recognize that it is in the form of sigmoid function
$\sigma\left( f(x) \right)$ where
$f(x) = - \left( \mu_{1}^{- 1} - \mu_{2}^{- 1} \right)k$. Then it is
easy to find the result according to a well-known property of sigmoid
function which states that
$\sigma\left( f(x) \right) + \sigma\left( - f(x) \right) = 1.$

By simultaneously solving Eq. 8.1 and
$P\left( H_{1} \middle| \mu_{0} + k \right) + P\left( H_{1} \middle| \mu_{0} - k \right) = 1$
as calculated above, we have

$$\left\{ \begin{array}{r}
P\left( H_{1}|X = \mu_{0} + k \right) = P\left( H_{2}|X = \mu_{0} - k \right) \\
P\left( H_{1}|X = \mu_{0} - k \right) = P\left( H_{2}|X = \mu_{0} + k \right)
\end{array}, \right.\ $$

or equivalently

$$\left\{ \begin{array}{r}
P\left( H_{1}|X = x \right) = P\left( H_{2}|2\mu_{0} - x \right) \\
P\left( H_{2}|X = x \right) = P\left( H_{1}|2\mu_{0} - x \right)
\end{array} \right.\ .$$

According to CLT,

$X\sim Normal\left( \mu_{0},\frac{\mu_{0}^{2}}{n} \right),$ as
$n \rightarrow \infty.$

Define a new variable
$Y = \frac{X - \mu_{0}}{\sqrt{\frac{\mu_{0}^{2}}{n}}}.$ Thus, $Y$
follows standard normal distribution as $n \rightarrow \infty.$ Hence,
the expectation of $P\left( H_{1}|X \right)$ can be calculated as

$$
\begin{align*}
E\left( P\left( H_{1}|X \right) \right) &\approx \int_{- \infty}^{\infty}{\left| \frac{dy}{dx} \right| \times \phi\left( \frac{x - \mu_{0}}{\sqrt{\frac{\mu_{0}^{2}}{n}}} \right) \times P\left( H_{1} \middle| X = x \right)}dx \\
&= \int_{- \infty}^{\infty}{\frac{1}{\sqrt{\frac{\mu_{0}^{2}}{n}}} \times \phi\left( \frac{\mu_{0} - x}{\sqrt{\frac{\mu_{0}^{2}}{n}}} \right) \times P\left( H_{2} \middle| X = 2\mu_{0} - x \right)}dx \\
&= \int_{- \infty}^{\infty}{\frac{1}{\sqrt{\frac{\mu_{0}^{2}}{n}}} \times \phi\left( \frac{(2\mu_{0} - x) - \mu_{0}}{\sqrt{\frac{\mu_{0}^{2}}{n}}} \right) \times P\left( H_{2} \middle| X = 2\mu_{0} - x \right)dx} \\
&\approx E\left( P\left( H_{1}|X \right) \right).
\end{align*}
$$

**Solution 2.**

According to the problem statement, we have

$$
\begin{align*}
&P\left( H_{1} \middle| X_{1} = x_{1},\ldots,X_{n} = x_{n} \right) \\
&= \frac{\left( \frac{1}{\mu_{1}} \right)^{n}e^{- \frac{1}{\mu_{1}}\left( x_{1} + \ldots + x_{n} \right)}}{e^{- \frac{1}{\mu_{1}}\left( x_{1} + \ldots + x_{n} \right)} + e^{- \frac{1}{\mu_{2}}\left( x_{1} + \ldots + x_{n} \right)}} \\
&= \frac{1}{1 + {\left( \frac{\mu_{1}}{\mu_{2}} \right)^{n}e}^{\left( \frac{1}{\mu_{1}} - \frac{1}{\mu_{2}} \right)\left( x_{1} + \ldots + x_{n} \right)}} \\
&= \frac{1}{1 + e^{\left( \frac{1}{\mu_{1}} - \frac{1}{\mu_{2}} \right)\left( x_{1} + \ldots + x_{n} \right) + n\log\left( \frac{\mu_{1}}{\mu_{2}} \right)}}.
\end{align*}
$$

As given in the problem statement,

$$\mu_{0} = \frac{\log\left( \frac{\mu_{2}}{\mu_{1}} \right)}{\frac{1}{\mu_{1}} - \frac{1}{\mu_{2}}}.$$

So

$$n\log\left( \frac{\mu_{1}}{\mu_{2}} \right) = - \mu_{0}\left( \frac{1}{\mu_{1}} - \frac{1}{\mu_{2}} \right).$$

Substituting $n\log\left( \frac{\mu_{2}}{\mu_{1}} \right)$ for
$\mu_{0}\left( \frac{1}{\mu_{1}} - \frac{1}{\mu_{2}} \right)$ into the
above, we have\
$$P\left( H_{1} \middle| X_{1} = x_{1},\ldots,X_{n} = x_{n} \right) = \frac{1}{1 + e^{\left( \frac{1}{\mu_{1}} - \frac{1}{\mu_{2}} \right)\left( x_{1} + \ldots + x_{n} - n\mu_{0} \right)}}.$$

Define $Y_{n} = X_{1} + \ldots + X_{n}$. According to CLT, apparently

$$Y_{n}\dot{\sim}Normal\left( n\mu_{0},n\mu_{0}^{2} \right).$$

Define $Z_{n} = P\left( H_{1} \middle| X_{1},\ldots,X_{n} \right),$ we
have

$$
{Z_{n} = \frac{1}{1 + e^{\left( \frac{1}{\mu_{1}} - \frac{1}{\mu_{2}} \right)\left( X_{1} + \ldots + X_{n} - n\mu_{0} \right)}}
}{= \frac{1}{1 + e^{\left( \frac{1}{\mu_{1}} - \frac{1}{\mu_{2}} \right)\left( Y_{n} - n\mu_{0} \right)}}.
}
$$

Because
$Y_{n} - n\mu_{0}\dot{\sim}Normal\left( 0,n\mu_{0}^{2} \right),$ using a
similar logic in Problem 8.3 above, it is not difficult to see that it
holds that the two models given in the problem statement are equally
wrong.
