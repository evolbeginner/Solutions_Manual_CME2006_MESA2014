**Solutions.**

We provide two solutions to (b). The first solves it combinatorically,
while the second uses an approach based on infinite series. Note that
as (b) seems to us a bit unclear and as we interpret it our answer to (b) is slightly different from what is given in the problem statement:
an unbiased estimator of the parameter $\theta$ in negative binomial distribution is $\widetilde{\theta} = \frac{x - 1}{N - 1}$ instead of $\frac{x}{N - 1}$.


**Solution 1.**

a\) Under the model of binomial distribution:

$$
\begin{align*}
E(X) &= \sum_{i = 0}^{n}{i \cdot P(X = i)} \\
&= \sum_{i = 1}^{n}{i\begin{pmatrix}
n \\
i \\
\end{pmatrix}\theta^{i}(1 - \theta)^{n - i}} \\
&= n\theta\sum_{i = 1}^{n}{\begin{pmatrix}
n - 1 \\
i - 1 \\
\end{pmatrix}\theta^{i - 1}(1 - \theta)^{(n - 1) - (i - 1)}} \\
&= n\theta\sum_{j = 0}^{n - 1}{\begin{pmatrix}
n - 1 \\
j \\
\end{pmatrix}\theta^{j}(1 - \theta)^{(n - 1) - j}} \\
&= n\theta(1 - \theta + \theta)^{n - 1} \\
&= n\theta.
\end{align*}
$$

b\) Under the model of negative binomial distribution, given the
context, the number of heads $x = 9$ is fixed and the number of tails,
denoted as $Y$, follows a negative binomial distribution NB(9,
$\theta$). Hence, the number of total tosses can be denoted as
$N = Y + 9$. In the example given in the problem, $N = 12$.

It should be noted however that an unbiased estimator of $\theta$ is
$\widetilde{\theta} = \frac{x - 1}{N - 1}$. In other words,
$E\left( \widetilde{\theta} \right) = E\left( \frac{x - 1}{n - 1} \right) = \theta$
(*Negative Binomial Distribution*, 2023). This is different from the
"answer" given in the problem. The following shows why
$\widetilde{\theta} = \frac{x - 1}{N - 1}$ instead of $\frac{x}{N - 1}$
is an unbiased estimator. The expectation of
$\widetilde{\theta} = \frac{x - 1}{N - 1}$ may be calculated as follows:

$$
\begin{align*}
E(\widetilde{\theta}) &= \sum_{n = x}^{\infty}{\frac{x - 1}{n - 1}\begin{pmatrix}
n - 1 \\
x - 1 \\
\end{pmatrix}\theta^{x}(1 - \theta)^{n - x}} \\
&= \sum_{i = 0}^{\infty}{\frac{x - 1}{x + i - 1}\begin{pmatrix}
x + i - 1 \\
x - 1 \\
\end{pmatrix}\theta^{x}(1 - \theta)^{i}}\quad\text{(n = x + i)} \\
&= \sum_{i = 0}^{\infty}{\frac{x - 1}{x + i - 1}\begin{pmatrix}
x + i - 1 \\
i \\
\end{pmatrix}\theta^{x}(1 - \theta)^{i}}
\end{align*}
$$

By plugging

$$
\begin{pmatrix}
x + i - 1 \\
i \\
\end{pmatrix} = \frac{x + i - 1}{i}\begin{pmatrix}
x + i - 2 \\
i - 1 \\
\end{pmatrix}
$$

into the above, we have

$$
\begin{align*}
E\left( \widetilde{\theta} \right) &= \sum_{i = 0}^{\infty}{\frac{x - 1}{x + i - 1}\frac{x + i - 1}{i}\begin{pmatrix}
x + i - 2 \\
i - 1 \\
\end{pmatrix}\theta^{x}(1 - \theta)^{i}} \\
&= \sum_{i = 0}^{\infty}{\frac{x - 1}{i}\frac{(x + i - 2)!}{(i - 1)!(x - 1)!}\theta^{x}(1 - \theta)^{i}} \\
&= \sum_{i = 0}^{\infty}{\frac{1}{i(i - 1)!}\frac{(x + i - 2)!}{(x - 2)!}\theta^{x}(1 - \theta)^{i}} \\
&= \sum_{i = 0}^{\infty}{\frac{(x + i - 2)!}{i!(x - 2)!}\theta^{x}(1 - \theta)^{i}} \\
&= \sum_{i = 0}^{\infty}{\begin{pmatrix}
x + i - 2 \\
i \\
\end{pmatrix}\theta^{x}(1 - \theta)^{i}} \\
&= \theta\sum_{i = 0}^{\infty}{\begin{pmatrix}
x + i - 2 \\
i \\
\end{pmatrix}\theta^{x - 1}(1 - \theta)^{i}} \\
&= \theta\sum_{i = 0}^{\infty}{\begin{pmatrix}
x' + i - 1 \\
i \\
\end{pmatrix}\theta^{x'}(1 - \theta)^{i}}\quad\left( x' = x - 1 \right) \\
&= \theta \times 1 \\
&= \theta.
\end{align*}
$$

Hence, an unbiased estimator of $\theta$ under the negative binomial model
when 3 failures and 9 successes are observed is
$\frac{9 - 1}{12 - 1} = \frac{8}{11}$.

**Solution 2.**

\(b\) Define
$(x)_{n} = x \times (x - 1) \times \cdots \times (x - n + 1)$. This is
known as the falling and rising factorial (Graham et al., 1994). Hence,

<p align="center">
  <img src="img/5.2-1.png">
</p>

Consider the summation function

$$f(t) = \sum_{n = x}^{\infty}{(n - 2)_{x - 2}t^{n - x}}.$$

It follows that

$${f(t) = \sum_{n = x}^{\infty}\frac{d^{x - 2}}{dt^{x - 2}}\left( t^{n - 2} \right)
}{= \sum_{k = x - 2}^{\infty}\frac{d^{x - 2}}{dt^{x - 2}}\left( t^{k} \right).}$$

Define

$$g(y) = \frac{d^{x - 2}}{dt^{x - 2}}\left( t^{y} \right).$$

It is easy to see that

$$g(0) = g(1) = \ldots = g(k - 3) = 0.$$

Also, noting that $0 < t < 1 - \theta < 1$, the geometric series
$\sum_{k = 0}^{\infty}t^{k}$ can be calculated as

$$
\begin{align*}
\sum_{k = 0}^{\infty}t^{k} &= \lim_{n \rightarrow \infty}{(1 + t^{1} + t^{2} + \ldots t^{n})} \\
&= \frac{1}{1 - k}.
\end{align*}
$$

So $f(t)$ may be re-written as

$$
\begin{align*}
f(t) &= \sum_{k = 0}^{\infty}\frac{d^{x - 2}}{dt^{x - 2}}\left( t^{k} \right) \\
&= \frac{d^{x - 2}}{dt^{x - 2}}\left( \sum_{k = 0}^{\infty}t^{k} \right)\quad \text{(sum - differentiation order swapping)} \\
&= \frac{d^{x - 2}}{dt^{x - 2}}\left( \frac{1}{1 - t} \right) \\
&= - 1 \times ( - 1) \times ( - 2) \times \cdots \times (2 - x) \times (t - 1)^{1 - x}.
\end{align*}
$$

Noting that

$$\begin{align*}
( - 1) \times ( - 2) \times \cdots \times (2 - x) &= ( - 1)^{2 - x} \times (x - 2) \times (x - 1) \times \cdots \times 1 \\
&= ( - 1)^{2 - x}(x - 2)!
\end{align*}$$

and that

$$(t - 1)^{1 - x} = {( - 1)^{1 - x} \times (1 - t)}^{1 - x},$$

we have

$$\begin{align*}
f(t) &= {- 1 \times ( - 1)}^{2 - x}(x - 2)! \times {( - 1)^{1 - x} \times (1 - t)}^{1 - x} \\
&= (x - 2)!(1 - t)^{1 - x}.
\end{align*}$$

So

$$\begin{align*}
f(1 - \theta) &= \sum_{n = x}^{\infty}{(n - 2)_{x - 2}(1 - \theta)^{n - x}} \\
&= (x - 2)!\theta^{1 - x},
\end{align*}$$

$$f(\theta) = (x - 2)!{(1 - \theta)}^{1 - x}.$$

Plugging this into
$E\left( \widetilde{\theta} \right) = \frac{\theta^{x}}{(x - 2)!}\sum_{n = x}^{\infty}{(n - 2)_{x - 2}(1 - \theta)^{n - x}}$
(see above), we have

$${E\left( \widetilde{\theta} \right) = \frac{\theta^{x}}{(x - 2)!}(x - 2)!\theta^{1 - x}
}{= \theta.}$$
